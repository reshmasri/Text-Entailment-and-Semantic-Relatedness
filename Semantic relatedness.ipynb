{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PuEUzFNve0i-"
   },
   "outputs": [],
   "source": [
    "!pip install pytorch-pretrained-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "14_fQdNVgav2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "import logging\n",
    "import urllib\n",
    "import sys\n",
    "import os\n",
    "import zipfile\n",
    "from os.path import join, exists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LqQIfrbBgn7Z"
   },
   "outputs": [],
   "source": [
    "## Helper function to read data from URLS and store in dataframes\n",
    "\n",
    "def readData(file):\n",
    "    data = pd.read_csv(file, delimiter='\\t', header = None, skiprows=1)\n",
    "    data.columns = [\"pair_ID\", \"sentence_A\", \"sentence_B\", \"relatedness_score\", \"entailment_judgment\"]\n",
    "    return data\n",
    "\n",
    "def column_values_tolist(output_list,df,col_name):\n",
    "  for i in range(len(df)):\n",
    "    output_list.append(df[col_name][i])\n",
    "  return output_list\n",
    "\n",
    "def encode_class_labels(ColumnAsList):\n",
    "  for i in range(len(ColumnAsList)):\n",
    "    if ColumnAsList[i] == 'CONTRADICTION':\n",
    "      ColumnAsList [i] = 0\n",
    "    elif ColumnAsList [i] == 'NEUTRAL':\n",
    "      ColumnAsList [i] = 1\n",
    "    elif ColumnAsList [i] == \"ENTAILMENT\":\n",
    "      ColumnAsList [i] = 2\n",
    "    else:\n",
    "      pass\n",
    "  return ColumnAsList\n",
    "\n",
    "def get_sent_em(sentence):\n",
    "  text = sentence\n",
    "  # Add the special tokens.\n",
    "  marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "  # Split the sentence into tokens.\n",
    "  tokenized_text = tokenizer.tokenize(marked_text)\n",
    "  # Map the token strings to their vocabulary indeces.\n",
    "  indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "  segments_ids = [1] * len(tokenized_text)\n",
    "  tokens_tensor = torch.tensor([indexed_tokens])\n",
    "  segments_tensors = torch.tensor([segments_ids])\n",
    "  with torch.no_grad():\n",
    "    encoded_layers, _ = model(tokens_tensor, segments_tensors)\n",
    "  token_embeddings = torch.stack(encoded_layers, dim=0)\n",
    "  token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "  token_embeddings = token_embeddings.permute(1,0,2)\n",
    "  token_vecs = encoded_layers[11][0]\n",
    "  sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "  return(sentence_embedding)\n",
    "\n",
    "\n",
    "def get_bert_mean_pool(list_of_sentences):\n",
    "  #passing list of sentences to this function\n",
    "  output = list(map(get_sent_em,list_of_sentences))\n",
    "\n",
    "  ## returns output which is list of tensors which are bert sentences embeddings with Mean pooling over bert word embeddings\n",
    "  return output\n",
    "\n",
    "\n",
    "def absolute_listoflists(u,v):\n",
    "\n",
    "  ##returns abs value of u,v |u-v| where u, v are list of lists\n",
    "  assert len(u) == len(v)\n",
    "  inter_list = []\n",
    "  final_list = []\n",
    "  for i in range(len(u)):\n",
    "    for j in range(len(u[0])):\n",
    "      inter_list.append(abs(u[i][j]-v[i][j]))\n",
    "    final_list.append(inter_list)\n",
    "    inter_list = []\n",
    "  return final_list\n",
    "\n",
    "\n",
    "\n",
    "def final_embedding(u,v,auv):\n",
    "  ## returns concatenated final embeddings [u,v,|u-v|]\n",
    "  temp = u\n",
    "  for i in range(len(u)):\n",
    "    temp[i].extend(v[i])\n",
    "    temp[i].extend(auv[i])\n",
    "\n",
    "  return temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IStf26olgo4e"
   },
   "outputs": [],
   "source": [
    "def get_sbert_embeddings(dataframe):\n",
    "\n",
    "  sentence_A_list = []\n",
    "\n",
    "  sentence_A_list = column_values_tolist(sentence_A_list,dataframe,\"sentence_A\")\n",
    "\n",
    "  sentence_B_list = []\n",
    "\n",
    "  sentence_B_list = column_values_tolist(sentence_B_list,dataframe,\"sentence_B\")\n",
    "\n",
    "\n",
    "## getting mean pool sentence embeddings from bert word embeddings\n",
    "\n",
    "  sentence_A_meanpool_embedding = get_bert_mean_pool(sentence_A_list)\n",
    "\n",
    "  sentence_B_meanpool_embedding = get_bert_mean_pool(sentence_B_list)\n",
    "\n",
    "##output from above function is list of tensors and convertin them to list\n",
    "\n",
    "  sent_A_list = [sentence_A_meanpool_embedding[i].tolist()  for i in range(len(sentence_A_meanpool_embedding))]\n",
    "\n",
    "  sent_B_list = [sentence_B_meanpool_embedding[i].tolist()  for i in range(len(sentence_B_meanpool_embedding))]\n",
    "\n",
    "##To get |u-v| for all the sentence embeddings in the list\n",
    "\n",
    "  absoflists = absolute_listoflists(sent_A_list,sent_B_list)\n",
    "\n",
    "##Final siamese network sentence embeddings (u,v,|u-v|)\n",
    "\n",
    "  final_sent_embedding = final_embedding(sent_A_list,sent_B_list,absoflists)\n",
    "\n",
    "  return final_sent_embedding\n",
    "\n",
    "\n",
    "\n",
    "def df_for_task1(embeddings_list,dataframe):\n",
    "\n",
    "  labels = []\n",
    "  labels = column_values_tolist(labels,dataframe,\"entailment_judgment\")\n",
    "\n",
    "  labels_list = encode_class_labels(labels)\n",
    "\n",
    "  task1_df = pd.DataFrame.from_records(embeddings_list)\n",
    "\n",
    "  task1_df.insert(2304,column='Outcome',value=labels_list)\n",
    "\n",
    "  ## 0 for C , 1 for N and 2 for E\n",
    "\n",
    "  return task1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PRPALLWTk8wg"
   },
   "outputs": [],
   "source": [
    "def get_sbert_embeddings_task2(dataframe):\n",
    "\n",
    "  sentence_A_list = []\n",
    "\n",
    "  sentence_A_list = column_values_tolist(sentence_A_list,dataframe,\"sentence_A\")\n",
    "\n",
    "  sentence_B_list = []\n",
    "\n",
    "  sentence_B_list = column_values_tolist(sentence_B_list,dataframe,\"sentence_B\")\n",
    "\n",
    "\n",
    "## getting mean pool sentence embeddings from bert word embeddings\n",
    "\n",
    "  sentence_A_meanpool_embedding = get_bert_mean_pool(sentence_A_list)\n",
    "\n",
    "  sentence_B_meanpool_embedding = get_bert_mean_pool(sentence_B_list)\n",
    "\n",
    "##output from above function is list of tensors and convertin them to list\n",
    "\n",
    "  sent_A_list = [sentence_A_meanpool_embedding[i].tolist()  for i in range(len(sentence_A_meanpool_embedding))]\n",
    "\n",
    "  sent_B_list = [sentence_B_meanpool_embedding[i].tolist()  for i in range(len(sentence_B_meanpool_embedding))]\n",
    "\n",
    "  return sent_A_list,sent_B_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "CN1Rx4dgg3_r",
    "outputId": "2985b72f-31a6-4605-973d-b52cdfe738d6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 231508/231508 [00:00<00:00, 1298542.38B/s]\n",
      "100%|██████████| 407873900/407873900 [00:10<00:00, 39858978.78B/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): BertLayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load pre-trained model tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hy5RriU-g6xa"
   },
   "outputs": [],
   "source": [
    "train_data = \"http://www.site.uottawa.ca/~diana/csi5386/A2_2020/SICK_train.txt\"\n",
    "test_data =\"http://www.site.uottawa.ca/~diana/csi5386/A2_2020/SICK_test_annotated.txt\"\n",
    "validation_data =\"http://www.site.uottawa.ca/~diana/csi5386/A2_2020/SICK_trial.txt\"\n",
    "\n",
    "\n",
    "train = readData(train_data)\n",
    "validation = readData(validation_data)\n",
    "test = readData(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aQb2oiU3kyG_"
   },
   "outputs": [],
   "source": [
    "train_sentence_A , train_sentence_B = get_sbert_embeddings_task2(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fr1HhxIXBQzC"
   },
   "outputs": [],
   "source": [
    "relatedness_score = []\n",
    "\n",
    "relatedness_score = column_values_tolist(relatedness_score,train,\"relatedness_score\")\n",
    "\n",
    "#Converting list to numpy array\n",
    "rs_numpy = np.asarray(relatedness_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QC_ByorRnqKu"
   },
   "outputs": [],
   "source": [
    "# Computing cosine similarity of two sentences and storing them in a list first and then converting into numpy array format\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "Cosine_Similarity = [1-cosine(train_sentence_A[i], train_sentence_B[i]) for i in range(len(train_sentence_A))]\n",
    "\n",
    "Cosine_Similarity_num = np.asarray(Cosine_Similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1R6A8-Per-zF"
   },
   "outputs": [],
   "source": [
    "#Scaling relatedness score using min max scalar to range [0 ,1] \n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "relatedness_score_scaled = scaler.fit_transform(rs_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lqtc556ypEvI"
   },
   "source": [
    "SPEARMAN CORRELARION OF Approch - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "bE9poxzxuG6p",
    "outputId": "e30f265c-3418-43fe-9294-1f6e2e778736"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.591389536061306\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "#Spearman Correlation between relatedness score normalized and cosine similarity\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "coef , p = spearmanr(relatedness_score_scaled.squeeze(),Cosine_Similarity_num)\n",
    "\n",
    "print(coef)\n",
    "\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iDaCFaRUpK26"
   },
   "source": [
    "PEARSON CORRELATION OF Approch -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0MzrDI37vzHn",
    "outputId": "b5e01077-3270-4001-e222-ea14f38a5844"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6183053064192804"
      ]
     },
     "execution_count": 63,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pearson Correlation between relatedness score normalized and cosine similarity of sentences\n",
    "\n",
    "import numpy\n",
    "numpy.corrcoef(relatedness_score_scaled.squeeze(),Cosine_Similarity_num)[0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lm-8VN_rpPRl"
   },
   "source": [
    "MSE of predicted score and actual relatedness Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7V8DhK0Ey8Dm",
    "outputId": "4920e901-974d-457d-d03a-7bb798037db1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1074915262514193"
      ]
     },
     "execution_count": 64,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MSE between relatedness score normalized and cosine similarity of sentences\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mean_squared_error(relatedness_score_scaled.squeeze(),Cosine_Similarity_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z0sb2hfkF9RZ"
   },
   "outputs": [],
   "source": [
    "Cosine_Similarity = [float(\"{0:.3f}\".format(Cosine_Similarity[i])) for i in range(len(Cosine_Similarity))]\n",
    "labels = [float(\"{0:.3f}\".format(labels[i])) for i in range(len(labels))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 768
    },
    "colab_type": "code",
    "id": "4aYSFYRmLZ2A",
    "outputId": "b40b7ec4-18a2-4fbf-be55-8fae7607b7e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/32/e3d405806ea525fd74c2c79164c3f7bc0b0b9811f27990484c6d6874c76f/sentence-transformers-0.2.5.1.tar.gz (52kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 9.7MB/s \n",
      "\u001b[?25hCollecting transformers==2.3.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/10/aeefced99c8a59d828a92cc11d213e2743212d3641c87c82d61b035a7d5c/transformers-2.3.0-py3-none-any.whl (447kB)\n",
      "\u001b[K     |████████████████████████████████| 450kB 36.9MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (4.38.0)\n",
      "Requirement already satisfied, skipping upgrade: torch>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.4.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.18.2)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (0.22.2.post1)\n",
      "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: nltk in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (3.2.5)\n",
      "Requirement already satisfied, skipping upgrade: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0->sentence-transformers) (1.12.23)\n",
      "Collecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0MB 49.5MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0->sentence-transformers) (2.21.0)\n",
      "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0->sentence-transformers) (2019.12.20)\n",
      "Collecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
      "\u001b[K     |████████████████████████████████| 870kB 45.5MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sentence-transformers) (0.14.1)\n",
      "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from nltk->sentence-transformers) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: botocore<1.16.0,>=1.15.23 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.3.0->sentence-transformers) (1.15.23)\n",
      "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.3.0->sentence-transformers) (0.9.5)\n",
      "Requirement already satisfied, skipping upgrade: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.3.0->sentence-transformers) (0.3.3)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0->sentence-transformers) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0->sentence-transformers) (2019.11.28)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0->sentence-transformers) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0->sentence-transformers) (1.24.3)\n",
      "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.3.0->sentence-transformers) (7.1.1)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.23->boto3->transformers==2.3.0->sentence-transformers) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.23->boto3->transformers==2.3.0->sentence-transformers) (0.15.2)\n",
      "Building wheels for collected packages: sentence-transformers, sacremoses\n",
      "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sentence-transformers: filename=sentence_transformers-0.2.5.1-cp36-none-any.whl size=67076 sha256=e5558df01c17bf910be7c6d484d478b598d782d02f4dc9ed0cc5cbd40ff88fd6\n",
      "  Stored in directory: /root/.cache/pip/wheels/22/ca/b4/7ca542b411730a8840f8e090df2ddacffa1c4dd9f209684c19\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=f299f138cfa35485bd85210ce53e93dd2c5184f6064d03158a11a1ac7e8d7c83\n",
      "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
      "Successfully built sentence-transformers sacremoses\n",
      "Installing collected packages: sentencepiece, sacremoses, transformers, sentence-transformers\n",
      "Successfully installed sacremoses-0.0.38 sentence-transformers-0.2.5.1 sentencepiece-0.1.85 transformers-2.3.0\n"
     ]
    }
   ],
   "source": [
    "## Getting Pretrained Siamese architecture Sentence Embeddings\n",
    "\n",
    "pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "16dPIlH_LoRl",
    "outputId": "9a4b0b0e-0a3b-49f0-a277-f1d1cb9a453a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 405M/405M [00:24<00:00, 16.2MB/s]\n"
     ]
    }
   ],
   "source": [
    "## Instance of Model , here model which has bert and pooling layer top of it\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "juN88DuBL9HW"
   },
   "outputs": [],
   "source": [
    "## Storing Sentences in lists\n",
    "\n",
    "sentence_A_list = []\n",
    "\n",
    "sentence_A_list = column_values_tolist(sentence_A_list,train,\"sentence_A\")\n",
    "\n",
    "sentence_B_list = []\n",
    "\n",
    "sentence_B_list = column_values_tolist(sentence_B_list,train,\"sentence_B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HQvsIVpgLwt0"
   },
   "outputs": [],
   "source": [
    "## getting sentence embeddings from instantiated Model\n",
    "\n",
    "SentA_pretrained_embeddings = model.encode(sentence_A_list)\n",
    "\n",
    "SentB_pretrained_embeddings = model.encode(sentence_B_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BtkKwD0rMs2h"
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "Cosine_Similarity_withpretrained = [1-cosine(SentA_pretrained_embeddings[i], SentB_pretrained_embeddings[i]) for i in range(len(SentA_pretrained_embeddings))]\n",
    "\n",
    "Cosine_Similarity_withpretrained_num = np.asarray(Cosine_Similarity_withpretrained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SAxZAfdFqRq0"
   },
   "source": [
    "SPEARMAN CORRELATION TRAIN : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "lljpQA8DN28H",
    "outputId": "f8fdfc84-ca36-42a7-a289-0318bdece26c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.732856992616356\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "coef , p = spearmanr(relatedness_score_scaled.squeeze(),Cosine_Similarity_withpretrained_num)\n",
    "\n",
    "print(coef)\n",
    "\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "on7GMCtlqk74"
   },
   "source": [
    "PEARSON TRAIN :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bY1rWhy5OGx-",
    "outputId": "2b2a3ede-fc93-4e4f-c0cb-ddfae36d3232"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7314216694251019"
      ]
     },
     "execution_count": 160,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "numpy.corrcoef(relatedness_score_scaled.squeeze(),Cosine_Similarity_withpretrained_num)[0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TQp190M7qw9a"
   },
   "source": [
    "MSE TRAIN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vloynWueOMfA",
    "outputId": "e91c4019-e2bf-440d-e92e-e63ec9f584fd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03764292691783672"
      ]
     },
     "execution_count": 158,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mean_squared_error(relatedness_score_scaled.squeeze(),Cosine_Similarity_withpretrained_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sz0jZE6OPC8f"
   },
   "outputs": [],
   "source": [
    "train_output_scores = scaler.inverse_transform(Cosine_Similarity_withpretrained_num.reshape(4500,1)).squeeze().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z0mz8Nu1Srvq"
   },
   "outputs": [],
   "source": [
    "## Getting train and Val Sentences\n",
    "\n",
    "test_sentence_A = []\n",
    "test_sentence_A = column_values_tolist(test_sentence_A,test,\"sentence_A\")\n",
    "\n",
    "test_sentence_B = []\n",
    "test_sentence_B = column_values_tolist(test_sentence_B,test,\"sentence_B\")\n",
    "\n",
    "test_relatedness_score = []\n",
    "test_relatedness_score = column_values_tolist(test_relatedness_score,test,\"relatedness_score\")\n",
    "\n",
    "val_sentence_A = []\n",
    "val_sentence_A = column_values_tolist(val_sentence_A,validation,\"sentence_A\")\n",
    "\n",
    "val_sentence_B = []\n",
    "val_sentence_B = column_values_tolist(val_sentence_B,validation,\"sentence_B\")\n",
    "\n",
    "val_relatedness_score = []\n",
    "val_relatedness_score = column_values_tolist(val_relatedness_score,validation,\"relatedness_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S-8FGHYPTFLq"
   },
   "outputs": [],
   "source": [
    "## Sentence Embeddings of test and val\n",
    "\n",
    "SentA_pretrained_embeddings_test = model.encode(test_sentence_A)\n",
    "\n",
    "SentB_pretrained_embeddings_test = model.encode(test_sentence_B)\n",
    "\n",
    "SentA_pretrained_embeddings_val = model.encode(val_sentence_A)\n",
    "\n",
    "SentB_pretrained_embeddings_val = model.encode(val_sentence_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G9c1X4LOUMCo"
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "Cosine_Similarity_withpretrained_test = [1-cosine(SentA_pretrained_embeddings_test[i], SentB_pretrained_embeddings_test[i]) for i in range(len(SentA_pretrained_embeddings_test))]\n",
    "\n",
    "Cosine_Similarity_withpretrained_num_test = np.asarray(Cosine_Similarity_withpretrained_test)\n",
    "\n",
    "\n",
    "Cosine_Similarity_withpretrained_val = [1-cosine(SentA_pretrained_embeddings_val[i], SentB_pretrained_embeddings_val[i]) for i in range(len(SentA_pretrained_embeddings_val))]\n",
    "\n",
    "Cosine_Similarity_withpretrained_num_val = np.asarray(Cosine_Similarity_withpretrained_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "juY4AxPoWnNJ"
   },
   "outputs": [],
   "source": [
    "\n",
    "test_relatedness_score_scaled = scaler.fit_transform(np.asarray(test_relatedness_score).reshape(4927,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eHmi3f7KrDVE"
   },
   "source": [
    "TEST SPEARMAN : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "nRb-9fvzV4FN",
    "outputId": "30da2875-6023-4e7a-f2e5-20b62f122778"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7291454051597714\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "coef , p = spearmanr(test_relatedness_score_scaled.squeeze(),Cosine_Similarity_withpretrained_num_test)\n",
    "\n",
    "print(coef)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lOdEFTCmrGeN"
   },
   "source": [
    "TEST PEARSON Approch 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0KDrMgKHYUhf",
    "outputId": "637e90e3-edc2-4030-aa34-cb2efb480354"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7294967112365477"
      ]
     },
     "execution_count": 222,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "numpy.corrcoef(test_relatedness_score_scaled.squeeze(),Cosine_Similarity_withpretrained_num_test)[0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga7a3t1crrUm"
   },
   "source": [
    "MSE TEST Approch 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "T8m_8DgsYeVr",
    "outputId": "06b631b7-8086-42a1-f43d-c227d52d09de"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03691746826457032"
      ]
     },
     "execution_count": 223,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "mean_squared_error(test_relatedness_score_scaled.squeeze(),Cosine_Similarity_withpretrained_num_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K2pPOnnLZTLg"
   },
   "outputs": [],
   "source": [
    "\n",
    "val_relatedness_score_scaled = scaler.fit_transform(np.asarray(val_relatedness_score).reshape(500,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ok7xOUm6rSEf"
   },
   "source": [
    "VALIDATION SPEARMAN Approch 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PJg7P_nIZkG-",
    "outputId": "331df86b-91fd-43dc-f017-2de728b6f444"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7286048633682622\n"
     ]
    }
   ],
   "source": [
    "coef , p = spearmanr(val_relatedness_score_scaled.squeeze(),Cosine_Similarity_withpretrained_num_val)\n",
    "\n",
    "print(coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lJKrqWFArUhg"
   },
   "source": [
    "PEARSON COEFF VALIDATION :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "oK_jdV7YZtNO",
    "outputId": "2c08491c-72a6-499a-d432-06a04e8d60b4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7041413759964165"
      ]
     },
     "execution_count": 226,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "numpy.corrcoef(val_relatedness_score_scaled.squeeze(),Cosine_Similarity_withpretrained_num_val)[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Oy2zFXViZ0HA",
    "outputId": "dfc750cd-843a-4d37-aa00-ebf4b8fcb5ad"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03997629858469614"
      ]
     },
     "execution_count": 227,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "mean_squared_error(val_relatedness_score_scaled.squeeze(),Cosine_Similarity_withpretrained_num_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3ojv44fYZ6ME"
   },
   "outputs": [],
   "source": [
    "\"\"\"test_output_scores = scaler.inverse_transform(Cosine_Similarity_withpretrained_num_test.reshape(4927,1)).squeeze().tolist()\n",
    "test_output_scores = [float(\"{0:.3f}\".format(test_output_scores[i])) for i in range(len(test_output_scores))]\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving results to Results.txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('final_report_test.data', 'rb') as filehandle:\n",
    "# read the data as binary data stream\n",
    "     entailment_judgment = pickle.load(filehandle)\n",
    "#      print(entailment_judgment)\n",
    "\n",
    "with open('final_report_scores.data', 'rb') as filehandle:\n",
    "# read the data as binary data stream\n",
    "     relatedness_score = pickle.load(filehandle)\n",
    "#      print(relatedness_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def readtest(file):\n",
    "    data = pd.read_csv(file, delimiter='\\t', header = None, skiprows=1)\n",
    "    data.columns = [\"pair_ID\", \"sentence_A\", \"sentence_B\"]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data Shape : (4927, 3)\n"
     ]
    }
   ],
   "source": [
    "test_data =\"http://www.site.uottawa.ca/~diana/csi5386/A2_2020/SICK_test.txt\"\n",
    "test = readtest(test_data)\n",
    "print(\"Test Data Shape :\",test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data Shape : (4927, 5)\n",
      "Test Data Shape : (4927, 3)\n"
     ]
    }
   ],
   "source": [
    "test[\"entailment_judgment\"] = entailment_judgment\n",
    "test[\"relatedness_score\"] = relatedness_score\n",
    "print(\"Test Data Shape :\",test.shape)\n",
    "test = test.drop([\"sentence_A\",\"sentence_B\"],axis = 1)\n",
    "print(\"Test Data Shape :\",test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.savetxt(\"Results.txt\", test.values, delimiter=\"\\t\", newline = \"\\n\", fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TASK-2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
